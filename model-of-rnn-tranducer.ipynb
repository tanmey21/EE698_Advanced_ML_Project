{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":354563,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":295745,"modelId":316353},{"sourceId":354565,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":295747,"modelId":316355}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nimport nltk\nfrom nltk.corpus import cmudict\n\n# 1. Install and imports\ntry:\n    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import torch.optim as optim\n    from torch.utils.data import Dataset, DataLoader, random_split\n    from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n    import torchaudio\nexcept ModuleNotFoundError:\n    raise ModuleNotFoundError(\"Please install dependencies: pip install torch torchaudio nltk\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:52:12.528936Z","iopub.execute_input":"2025-04-24T07:52:12.529147Z","iopub.status.idle":"2025-04-24T07:52:18.237181Z","shell.execute_reply.started":"2025-04-24T07:52:12.529128Z","shell.execute_reply":"2025-04-24T07:52:18.236532Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Data preprocessing\nnltk.download('cmudict', quiet=True)\ncmu_entries = cmudict.entries()\n\ndef build_vocab(tokens):\n    vocab = ['<pad>'] + sorted(tokens)\n    return {t:i for i,t in enumerate(vocab)}, vocab\n\n# Prepare pairs and simple curriculum: filter by max length\nMAX_LEN = 15\nchars, phs = set(), set()\npairs = []\nfor w, pron in cmu_entries:\n    w = w.lower()\n    if len(w) <= MAX_LEN:\n        pairs.append((list(w), pron))\n        chars.update(w)\n        phs.update(pron)\nchar2id, id2char = build_vocab(chars)\nph2id, id2ph = build_vocab(phs.union({'<blank>'}))\nblank_id = ph2id['<blank>']\n\npairs_ids = [([char2id[c] for c in w], [ph2id[p] for p in pron]) for w, pron in pairs]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:52:18.237932Z","iopub.execute_input":"2025-04-24T07:52:18.238381Z","iopub.status.idle":"2025-04-24T07:52:19.961292Z","shell.execute_reply.started":"2025-04-24T07:52:18.238343Z","shell.execute_reply":"2025-04-24T07:52:19.960706Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"#Checking the number of unique phonemes token\nunique_values_count = len(set(ph2id.values()))\nprint(unique_values_count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:54:16.602242Z","iopub.execute_input":"2025-04-24T07:54:16.602923Z","iopub.status.idle":"2025-04-24T07:54:16.606677Z","shell.execute_reply.started":"2025-04-24T07:54:16.602899Z","shell.execute_reply":"2025-04-24T07:54:16.606057Z"}},"outputs":[{"name":"stdout","text":"72\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Dataset & DataLoader\nclass CMUDictDataset(Dataset):\n    def __init__(self, data): self.data = data\n    def __len__(self): return len(self.data)\n    def __getitem__(self, idx): return self.data[idx]\n\ndef collate_fn(batch):\n    x_seq = [torch.tensor(x, dtype=torch.long) for x,_ in batch]\n    y_seq = [torch.tensor(y, dtype=torch.long) for _,y in batch]\n    x_lens = torch.tensor([len(x) for x in x_seq], dtype=torch.long)\n    y_lens = torch.tensor([len(y) for y in y_seq], dtype=torch.long)\n    x_pad = pad_sequence(x_seq, True, char2id['<pad>'])\n    y_pad = pad_sequence(y_seq, True, blank_id)\n    return x_pad, x_lens, y_pad, y_lens\n\ndataset = CMUDictDataset(pairs_ids)\ntrain_size = int(0.8 * len(dataset))\ntest_size = len(dataset) - train_size\ntrain_ds, test_ds = random_split(dataset, [train_size, test_size])\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn)\ntest_loader  = DataLoader(test_ds, batch_size=64, shuffle=False, collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:52:19.961922Z","iopub.execute_input":"2025-04-24T07:52:19.962138Z","iopub.status.idle":"2025-04-24T07:52:19.992079Z","shell.execute_reply.started":"2025-04-24T07:52:19.962121Z","shell.execute_reply":"2025-04-24T07:52:19.991294Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, vocab_size, embed_dim=256, hid_dim=512, num_layers=3, dropout=0.3):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=char2id['<pad>'])\n        self.lstm = nn.LSTM(embed_dim, hid_dim, num_layers=num_layers,\n                            batch_first=True, bidirectional=True, dropout=dropout)\n        self.out_dim = hid_dim * 2\n        self.ctc_fc = nn.Linear(self.out_dim, len(ph2id))\n    def forward(self, x, lengths):\n        e = self.embed(x)\n        packed = pack_padded_sequence(e, lengths.cpu(), True, False)\n        out, _ = self.lstm(packed)\n        out, _ = pad_packed_sequence(out, True)\n        return out\n    def ctc_logits(self, enc_out):\n        return self.ctc_fc(enc_out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:52:19.994787Z","iopub.execute_input":"2025-04-24T07:52:19.995000Z","iopub.status.idle":"2025-04-24T07:52:20.000713Z","shell.execute_reply.started":"2025-04-24T07:52:19.994983Z","shell.execute_reply":"2025-04-24T07:52:19.999944Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class Predictor(nn.Module):\n    def __init__(self, vocab_size, embed_dim=256, hid_dim=512, num_layers=2, dropout=0.3):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim)\n        self.lstm = nn.LSTM(embed_dim, hid_dim, num_layers=num_layers,\n                            batch_first=True, dropout=dropout)\n        self.hid_dim = hid_dim\n    def forward(self, y):\n        e = self.embed(y)\n        out, _ = self.lstm(e)\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:52:20.007098Z","iopub.execute_input":"2025-04-24T07:52:20.007305Z","iopub.status.idle":"2025-04-24T07:52:20.014734Z","shell.execute_reply.started":"2025-04-24T07:52:20.007289Z","shell.execute_reply":"2025-04-24T07:52:20.014010Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class Joiner(nn.Module):\n    def __init__(self, enc_dim, pred_dim, joint_dim=512):\n        super().__init__()\n        self.e_proj = nn.Linear(enc_dim, joint_dim)\n        self.p_proj = nn.Linear(pred_dim, joint_dim)\n        self.act = nn.ReLU()\n        self.fc = nn.Linear(joint_dim, len(ph2id))\n    def forward(self, e, p):\n        return self.fc(self.act(self.e_proj(e) + self.p_proj(p)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:52:20.015400Z","iopub.execute_input":"2025-04-24T07:52:20.015657Z","iopub.status.idle":"2025-04-24T07:52:20.031624Z","shell.execute_reply.started":"2025-04-24T07:52:20.015635Z","shell.execute_reply":"2025-04-24T07:52:20.031014Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class RNNTransducer(nn.Module):\n    def __init__(self, encoder, predictor, joiner, blank_id):\n        super().__init__()\n        self.encoder = encoder\n        self.predictor = predictor\n        self.joiner = joiner\n        self.blank_id = blank_id\n    def forward(self, x, x_lens, y, y_lens):\n        e_out = self.encoder(x, x_lens)          # [B,T,2H]\n        B,T,_ = e_out.size()\n        blank_col = torch.full((B,1), self.blank_id, dtype=y.dtype, device=y.device)\n        y_in = torch.cat([blank_col, y],1)\n        p_out = self.predictor(y_in)             # [B,U+1,H]\n        # expand for joiner\n        e_e = e_out.unsqueeze(2)\n        p_e = p_out.unsqueeze(1)\n        j = self.joiner(e_e, p_e)                # [B,T,U+1,V]\n        return j","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:52:20.032327Z","iopub.execute_input":"2025-04-24T07:52:20.032523Z","iopub.status.idle":"2025-04-24T07:52:20.044362Z","shell.execute_reply.started":"2025-04-24T07:52:20.032509Z","shell.execute_reply":"2025-04-24T07:52:20.043816Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# instantiate and scheduler\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nenc = Encoder(len(char2id)).to(device)\npred = Predictor(len(ph2id)).to(device)\njoi = Joiner(enc.out_dim, pred.hid_dim).to(device)\nmodel = RNNTransducer(enc,pred,joi,blank_id).to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nscheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\nctc_loss = nn.CTCLoss(blank=blank_id, zero_infinity=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:52:20.044966Z","iopub.execute_input":"2025-04-24T07:52:20.045176Z","iopub.status.idle":"2025-04-24T07:52:22.864199Z","shell.execute_reply.started":"2025-04-24T07:52:20.045160Z","shell.execute_reply":"2025-04-24T07:52:22.863558Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# Loss and training with auxiliary CTC\nALPHA = 0.3\n\ndef total_loss_fn(x, xl, y, yl, logits):\n    # RNN-T loss\n    lp = F.log_softmax(logits, dim=-1)\n    rnnt = torchaudio.functional.rnnt_loss(\n        lp,\n        y.to(torch.int32),\n        xl.to(torch.int32),\n        yl.to(torch.int32),\n        blank=blank_id\n    )\n    # CTC loss on encoder\n    # enc_out = enc(x, xl)\n    # ctc_logits = enc.ctc_logits(enc_out).log_softmax(-1)\n    # T = ctc_logits.size(1)\n    # input_len = torch.full((ctc_logits.size(0),), T, dtype=torch.long, device=device)\n    # ctc = ctc_loss(\n    #     ctc_logits.permute(1,0,2),  # T x B x V\n    #     y,\n    #     input_len,\n    #     yl\n    # )\n    return rnnt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:52:22.864951Z","iopub.execute_input":"2025-04-24T07:52:22.865254Z","iopub.status.idle":"2025-04-24T07:52:22.869590Z","shell.execute_reply.started":"2025-04-24T07:52:22.865236Z","shell.execute_reply":"2025-04-24T07:52:22.868954Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# beam search width\ndef beam_search(enc_out, width=5):\n    # enc_out: [T, D]\n    if enc_out.dim() == 3:\n        assert enc_out.size(0) == 1, \"batch size >1 not supported\"\n        enc_out = enc_out[0]\n    T, D = enc_out.size()\n    device = enc_out.device\n    # Each beam: (score, hypothesis list)\n    beams = [(0.0, [])]\n    for t in range(T):  # iterate over time steps\n        all_cands = []\n        for score, hyp in beams:\n            # Prepare predictor input: blank + current hypothesis\n            y_input = torch.tensor([[blank_id] + hyp], dtype=torch.long, device=device)\n            p_out = pred(y_input)                # [1, U+1, D]\n            pred_vec = p_out[:, -1, :]          # [1, D]\n            # Encoder output at time t\n            enc_vec = enc_out[t].unsqueeze(0)   # [1, D]\n            # Joint network\n            joint = torch.relu(joi.e_proj(enc_vec) + joi.p_proj(pred_vec))  # [1, J]\n            logp = F.log_softmax(joi.fc(joint).squeeze(0), dim=-1)           # [V]\n            topk = torch.topk(logp, width)\n            for lp, tok in zip(topk.values, topk.indices):\n                new_hyp = hyp + ([tok.item()] if tok.item() != blank_id else [])\n                all_cands.append((score + lp.item(), new_hyp))\n        # Prune to top-k beams\n        beams = sorted(all_cands, key=lambda x: x[0], reverse=True)[:width]\n    # Return the hypothesis with highest score\n    return beams[0][1]\n    beams=[(0,[])]\n    for t in range(enc_out.size(1)):\n        new=[]\n        for sc,h in beams:\n            y = torch.tensor([[blank_id]+h], device=device)\n            p = pred(y)[:, -1, :]\n            e = enc_out[0,t].unsqueeze(0)\n            j = torch.relu(joi.e_proj(e)+joi.p_proj(p))\n            logp = F.log_softmax(joi.fc(j).squeeze(0),-1)\n            top = torch.topk(logp, width)\n            for lp,tk in zip(top.values,top.indices):\n                lst = h+[tk.item()] if tk.item()!=blank_id else h\n                new.append((sc+lp.item(),lst))\n        beams=sorted(new, key=lambda x: x[0], reverse=True)[:width]\n    return beams[0][1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:52:22.870370Z","iopub.execute_input":"2025-04-24T07:52:22.870555Z","iopub.status.idle":"2025-04-24T07:52:22.897124Z","shell.execute_reply.started":"2025-04-24T07:52:22.870541Z","shell.execute_reply":"2025-04-24T07:52:22.896386Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Training loop\n\ndef train(epochs=15):\n    for ep in range(1,epochs+1):\n        model.train(); tot, cnt = 0,0\n        for b,(x,xl,y,yl) in enumerate(train_loader,1):\n            x,xl,y,yl = x.to(device),xl.to(device),y.to(device),yl.to(device)\n            logits = model(x,xl,y,yl)\n            loss = total_loss_fn(x, xl, y, yl, logits)\n            optimizer.zero_grad(); loss.backward(); optimizer.step()\n            tot += loss.item(); cnt+=1\n            if b%100==0:\n                print(f\"Ep{ep} B{b}/{len(train_loader)} Loss={loss.item():.3f}\")\n        scheduler.step()\n        print(f\"Epoch{ep} AvgLoss={tot/cnt:.3f} LR={scheduler.get_last_lr()[0]:.5f}\")\n        evaluate_cer('Train',train_loader)\n        evaluate_cer('Test', test_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:52:22.898013Z","iopub.execute_input":"2025-04-24T07:52:22.898221Z","iopub.status.idle":"2025-04-24T07:52:22.912882Z","shell.execute_reply.started":"2025-04-24T07:52:22.898205Z","shell.execute_reply":"2025-04-24T07:52:22.912172Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# CER and single-word decode\n\ndef edit_distance(a,b):\n    dp=[[i if j==0 else (j if i==0 else 0) for j in range(len(b)+1)] for i in range(len(a)+1)]\n    for i in range(1,len(a)+1):\n        for j in range(1,len(b)+1):\n            dp[i][j]=dp[i-1][j-1] if a[i-1]==b[j-1] else 1+min(dp[i-1][j],dp[i][j-1],dp[i-1][j-1])\n    return dp[-1][-1]\ndef cer(ref,hyp): return edit_distance(ref,hyp)/len(ref) if ref else 0\n\ndef evaluate_cer(name,loader):\n    model.eval(); tot,ct=0,0\n    with torch.no_grad():\n        for x,xl,y,yl in loader:\n            x,xl,y,yl=x.to(device),xl.to(device),y.to(device),yl.to(device)\n            eo=enc(x,xl)[0]\n            pred_ids=beam_search(eo, width=5)\n            true=y[0,:yl[0]].tolist();r=[id2ph[i] for i in true]\n            h=[id2ph[i] for i in pred_ids]\n            tot+=cer(r,h); ct+=1\n    print(f\"{name} CER={tot/ct:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:52:22.913447Z","iopub.execute_input":"2025-04-24T07:52:22.913624Z","iopub.status.idle":"2025-04-24T07:52:22.930070Z","shell.execute_reply.started":"2025-04-24T07:52:22.913610Z","shell.execute_reply":"2025-04-24T07:52:22.929456Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def main():\n    train(3)\n    for w in ['hello','world','chatgpt']:\n        eo=enc(torch.tensor([[char2id.get(c,'<pad>') for c in w]],device=device),\n                 torch.tensor([len(w)],device=device))\n        phs=[id2ph[i] for i in beam_search(eo,5)]\n        print(f\"{w} -> {' '.join(phs)}\")\n\nif __name__=='__main__': main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save model state\ntorch.save(model.state_dict(), \"rnn_transducer_wo_ctc.pth\")\nprint(\"Model saved to rnn_transducer_wo_ctc.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-23T18:28:53.184149Z","iopub.execute_input":"2025-04-23T18:28:53.184704Z","iopub.status.idle":"2025-04-23T18:28:53.354352Z","shell.execute_reply.started":"2025-04-23T18:28:53.184683Z","shell.execute_reply":"2025-04-23T18:28:53.353719Z"}},"outputs":[{"name":"stdout","text":"Model saved to rnn_transducer.pth\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Load model state\nmodel.load_state_dict(torch.load(\"/kaggle/input/rnn-t/pytorch/default/1/rnn_transducer.pth\", map_location=device))\nmodel.eval()\nprint(\"Model loaded from rnn_transducer.pth\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T07:52:34.017606Z","iopub.execute_input":"2025-04-24T07:52:34.017912Z","iopub.status.idle":"2025-04-24T07:52:34.733039Z","shell.execute_reply.started":"2025-04-24T07:52:34.017889Z","shell.execute_reply":"2025-04-24T07:52:34.732303Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_40/602078325.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"/kaggle/input/rnn-t/pytorch/default/1/rnn_transducer.pth\", map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"Model loaded from rnn_transducer.pth\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def predict_word(word):\n    # Preprocess word\n    chars = list(word.lower())\n    ids = [char2id.get(c, char2id['<pad>']) for c in chars]\n    x = torch.tensor([ids], dtype=torch.long, device=device)\n    x_lens = torch.tensor([len(ids)], dtype=torch.long, device=device)\n    # Encode\n    enc_out = enc(x, x_lens)[0]  # take [1,T,2H] -> [T,2H]\n    # Beam search decode\n    pred_ids = beam_search(enc_out, width=5)\n    # Map to phonemes\n    return [id2ph[i] for i in pred_ids]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T06:52:51.076699Z","iopub.execute_input":"2025-04-24T06:52:51.077396Z","iopub.status.idle":"2025-04-24T06:52:51.081881Z","shell.execute_reply.started":"2025-04-24T06:52:51.077375Z","shell.execute_reply":"2025-04-24T06:52:51.081237Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"predict_word(\"cement\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T06:53:09.763052Z","iopub.execute_input":"2025-04-24T06:53:09.763721Z","iopub.status.idle":"2025-04-24T06:53:09.795992Z","shell.execute_reply.started":"2025-04-24T06:53:09.763696Z","shell.execute_reply":"2025-04-24T06:53:09.795503Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"['S', 'AH0', 'M', 'EH1', 'N', 'T']"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"Words=['February','Pronunciation','Salmon','Handkerchief','Honest','Gnome']\nfor word in Words:\n    res=predict_word(word)\n    print(res)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-24T05:47:18.300211Z","iopub.execute_input":"2025-04-24T05:47:18.300762Z","iopub.status.idle":"2025-04-24T05:47:18.543023Z","shell.execute_reply.started":"2025-04-24T05:47:18.300742Z","shell.execute_reply":"2025-04-24T05:47:18.542330Z"}},"outputs":[{"name":"stdout","text":"['F', 'EH1', 'B', 'R', 'UW0', 'EH2', 'R', 'IY0']\n['P', 'R', 'OW0', 'N', 'AH2', 'N', 'S', 'IY0', 'EY1', 'SH', 'AH0', 'N']\n['S', 'AE1', 'L', 'M', 'AH0', 'N']\n['HH', 'AE1', 'NG', 'K', 'ER0', 'CH', 'IY2', 'F']\n['HH', 'OW1', 'N', 'AH0', 'S', 'T']\n['N', 'OW1', 'M']\n","output_type":"stream"}],"execution_count":23}]}
